{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒ² Forest Cover Type Prediction\n",
        "\n",
        "### Kaggle Competition Solution using Stacking & Blending Ensemble\n",
        "\n",
        "This notebook presents a robust machine learning pipeline for the [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction) competition. The goal is to predict the forest cover type (7 classes) based on cartographic variables.\n",
        "\n",
        "**Key Techniques:**\n",
        "- Advanced Feature Engineering (distance metrics, terrain analysis, domain features)\n",
        "- Target Encoding with Cross-Validation (leak-free)\n",
        "- Gradient Boosting Ensemble: LightGBM, XGBoost, CatBoost\n",
        "- Two Ensemble Strategies: Stacking (meta-learner) & Blending (weighted average)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "print(\"âœ“ All libraries loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "N_FOLDS = 7\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "The dataset contains cartographic variables for 30m x 30m cells from Roosevelt National Forest in Colorado. Features include:\n",
        "- **Elevation**: Elevation in meters\n",
        "- **Aspect**: Azimuth angle (0-360Â°)\n",
        "- **Slope**: Slope in degrees\n",
        "- **Distances**: Horizontal/vertical distance to hydrology, roadways, fire points\n",
        "- **Hillshade**: Illumination values at 9am, noon, and 3pm (summer solstice)\n",
        "- **Wilderness Areas**: 4 binary columns indicating wilderness area designation\n",
        "- **Soil Types**: 40 binary columns for soil type classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load train, test and sample submission files.\n",
        "    Searches common paths: current directory, Desktop, Kaggle input.\n",
        "    \"\"\"\n",
        "    search_paths = [\n",
        "        Path.cwd(),\n",
        "        Path.home() / \"Desktop\",\n",
        "        Path(\"/kaggle/input/forest-cover-type-prediction\")\n",
        "    ]\n",
        "    \n",
        "    for path in search_paths:\n",
        "        if not path.exists():\n",
        "            continue\n",
        "            \n",
        "        train_path = path / \"train.csv\"\n",
        "        test_path = path / \"test-full.csv\" if (path / \"test-full.csv\").exists() else path / \"test.csv\"\n",
        "        submission_path = path / \"full_submission.csv\" if (path / \"full_submission.csv\").exists() else path / \"sample_submission.csv\"\n",
        "        \n",
        "        if all(f.exists() for f in [train_path, test_path, submission_path]):\n",
        "            print(f\"âœ“ Data found in: {path}\")\n",
        "            return pd.read_csv(train_path), pd.read_csv(test_path), pd.read_csv(submission_path)\n",
        "    \n",
        "    raise FileNotFoundError(\"CSV files (train/test/submission) not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, test_df, sample_submission = load_data()\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(train_df[\"Cover_Type\"].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering\n",
        "\n",
        "We create domain-specific features that capture important relationships in the data:\n",
        "\n",
        "### Distance Features\n",
        "- **Euclidean & Manhattan distances** to hydrology sources\n",
        "- **Distance ratios** between hydrology, roads, and fire points\n",
        "\n",
        "### Terrain Features\n",
        "- **Elevation interactions** with vertical distance to hydrology\n",
        "- **Hillshade statistics**: mean and range across time points\n",
        "- **Aspect decomposition**: North-South and East-West components (cosine/sine transform)\n",
        "\n",
        "### Categorical Features\n",
        "- **Wilderness Area ID**: Collapsed from binary columns\n",
        "- **Soil Type ID**: Collapsed from 40 binary columns\n",
        "- **Climate Zone**: Derived from soil classification\n",
        "- **Stony Soil Indicator**: Based on domain knowledge of soil descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    \"\"\"\n",
        "    Create advanced features from raw data.\n",
        "    Returns processed dataframe and list of categorical feature names.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # --- Distance Features ---\n",
        "    # Euclidean and Manhattan distance to hydrology\n",
        "    df['Hydro_Euclidean'] = np.sqrt(\n",
        "        df['Horizontal_Distance_To_Hydrology']**2 + \n",
        "        df['Vertical_Distance_To_Hydrology']**2\n",
        "    )\n",
        "    df['Hydro_Manhattan'] = (\n",
        "        np.abs(df['Horizontal_Distance_To_Hydrology']) + \n",
        "        np.abs(df['Vertical_Distance_To_Hydrology'])\n",
        "    )\n",
        "    \n",
        "    # --- Elevation Interactions ---\n",
        "    df['Elev_minus_Hydro_Vert'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n",
        "    df['Elev_plus_Hydro_Vert'] = df['Elevation'] + df['Vertical_Distance_To_Hydrology']\n",
        "    \n",
        "    # --- Distance Ratios ---\n",
        "    eps = 1e-6  # Small constant to avoid division by zero\n",
        "    df['Hydro_Road_Ratio'] = (\n",
        "        df['Horizontal_Distance_To_Hydrology'] / \n",
        "        (df['Horizontal_Distance_To_Roadways'] + eps)\n",
        "    )\n",
        "    df['Hydro_Fire_Ratio'] = (\n",
        "        df['Horizontal_Distance_To_Hydrology'] / \n",
        "        (df['Horizontal_Distance_To_Fire_Points'] + eps)\n",
        "    )\n",
        "    df['Road_Fire_Ratio'] = (\n",
        "        df['Horizontal_Distance_To_Roadways'] / \n",
        "        (df['Horizontal_Distance_To_Fire_Points'] + eps)\n",
        "    )\n",
        "    \n",
        "    # --- Hillshade Statistics ---\n",
        "    hillshade_cols = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
        "    df[\"Hillshade_Mean\"] = df[hillshade_cols].mean(axis=1)\n",
        "    df[\"Hillshade_Range\"] = df[hillshade_cols].max(axis=1) - df[hillshade_cols].min(axis=1)\n",
        "    \n",
        "    # --- Aspect Decomposition (Circular Feature) ---\n",
        "    aspect_rad = np.deg2rad(df[\"Aspect\"])\n",
        "    df[\"North_South_Component\"] = np.cos(aspect_rad)\n",
        "    df[\"East_West_Component\"] = np.sin(aspect_rad)\n",
        "    \n",
        "    # --- Categorical ID Features ---\n",
        "    wilderness_cols = [c for c in df.columns if \"Wilderness_Area\" in c]\n",
        "    soil_cols = [c for c in df.columns if \"Soil_Type\" in c]\n",
        "    \n",
        "    if wilderness_cols:\n",
        "        df[\"Wilderness_ID\"] = np.argmax(df[wilderness_cols].values, axis=1) + 1\n",
        "    if soil_cols:\n",
        "        df[\"Soil_ID\"] = np.argmax(df[soil_cols].values, axis=1) + 1\n",
        "    \n",
        "    # --- Domain-Specific Features ---\n",
        "    # Climate zone derived from soil type classification\n",
        "    df['Climate_Zone'] = df['Soil_ID'].apply(lambda x: x // 1000)\n",
        "    \n",
        "    # Stony soils based on soil type descriptions from documentation\n",
        "    stony_soils = [3, 4, 5, 6, 9, 10, 11, 12, 13, 18, 22, 24, 26, 27, 28, 29, \n",
        "                   30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
        "    df['Is_Stony'] = df['Soil_ID'].isin(stony_soils).astype(int)\n",
        "    \n",
        "    # --- Interaction Features ---\n",
        "    df[\"Elev_x_Wilderness\"] = df[\"Elevation\"] * df[\"Wilderness_ID\"]\n",
        "    df[\"Slope_x_Wilderness\"] = df[\"Slope\"] * df[\"Wilderness_ID\"]\n",
        "    \n",
        "    # Define categorical features for target encoding\n",
        "    categorical_features = [\"Wilderness_ID\", \"Soil_ID\", \"Climate_Zone\"]\n",
        "    \n",
        "    # --- Clean up: Handle infinities and NaN ---\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    \n",
        "    return df, categorical_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Target Encoding\n",
        "\n",
        "Target encoding replaces categorical values with the mean target value for that category. To prevent data leakage, we use cross-validation target encoding with smoothing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def target_encode_cv(X_train, y_train, X_test, categorical_features, n_splits=7, smoothing=30):\n",
        "    \"\"\"\n",
        "    Apply target encoding with cross-validation to prevent leakage.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : DataFrame - Training features\n",
        "    y_train : Series - Training target\n",
        "    X_test : DataFrame - Test features\n",
        "    categorical_features : list - Columns to encode\n",
        "    n_splits : int - Number of CV folds\n",
        "    smoothing : float - Smoothing parameter (higher = more regularization)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, X_test with new target-encoded features\n",
        "    \"\"\"\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "    global_mean = y_train.mean()\n",
        "    \n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    \n",
        "    for feature in categorical_features:\n",
        "        if feature not in X_train.columns:\n",
        "            continue\n",
        "            \n",
        "        # Initialize arrays\n",
        "        te_train = np.zeros(len(X_train))\n",
        "        te_test_accumulated = np.zeros(len(X_test))\n",
        "        \n",
        "        for train_idx, val_idx in kf.split(X_train, y_train):\n",
        "            # Compute statistics on training fold only\n",
        "            fold_data = pd.DataFrame({\n",
        "                feature: X_train.iloc[train_idx][feature],\n",
        "                'target': y_train.iloc[train_idx]\n",
        "            })\n",
        "            stats = fold_data.groupby(feature)['target'].agg(['mean', 'count'])\n",
        "            \n",
        "            # Apply smoothing: blend category mean with global mean\n",
        "            smoothed_means = (\n",
        "                (stats['count'] * stats['mean'] + smoothing * global_mean) / \n",
        "                (stats['count'] + smoothing)\n",
        "            )\n",
        "            \n",
        "            # Encode validation fold\n",
        "            te_train[val_idx] = X_train.iloc[val_idx][feature].map(smoothed_means).fillna(global_mean).values\n",
        "            \n",
        "            # Accumulate test encodings\n",
        "            te_test_accumulated += X_test[feature].map(smoothed_means).fillna(global_mean).values\n",
        "        \n",
        "        # Add encoded features\n",
        "        X_train[f\"{feature}_TE\"] = te_train\n",
        "        X_test[f\"{feature}_TE\"] = te_test_accumulated / n_splits\n",
        "    \n",
        "    return X_train, X_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract target (convert to 0-indexed for sklearn)\n",
        "y = train_df[\"Cover_Type\"] - 1\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"Applying feature engineering...\")\n",
        "X, categorical_features = create_features(train_df.drop(columns=[\"Cover_Type\", \"Id\"]))\n",
        "X_test, _ = create_features(test_df.drop(columns=[\"Id\"]))\n",
        "\n",
        "# Apply target encoding\n",
        "print(\"Applying target encoding...\")\n",
        "X, X_test = target_encode_cv(X, y, X_test, categorical_features, n_splits=N_FOLDS)\n",
        "\n",
        "# Ensure same columns in train and test\n",
        "shared_columns = list(set(X.columns) & set(X_test.columns))\n",
        "X = X[shared_columns]\n",
        "X_test = X_test[shared_columns]\n",
        "\n",
        "print(f\"\\nâœ“ Final shapes: Train={X.shape}, Test={X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Training\n",
        "\n",
        "We train three gradient boosting models with optimized hyperparameters:\n",
        "- **LightGBM**: Fast, efficient, good with categorical features\n",
        "- **XGBoost**: Robust, handles regularization well\n",
        "- **CatBoost**: Excellent with categorical data, symmetric trees\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(X, y, X_test, params, model_type='lgbm', seed=42):\n",
        "    \"\"\"\n",
        "    Train a model using K-Fold cross-validation.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    oof_preds : Out-of-fold probability predictions (for stacking)\n",
        "    test_preds : Averaged test predictions\n",
        "    \"\"\"\n",
        "    n_classes = 7\n",
        "    oof_preds = np.zeros((len(X), n_classes))\n",
        "    test_preds = np.zeros((len(X_test), n_classes))\n",
        "    \n",
        "    kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X, y), total=N_FOLDS, desc=f\"{model_type.upper()}\")):\n",
        "        X_train_fold = X.iloc[train_idx]\n",
        "        X_val_fold = X.iloc[val_idx]\n",
        "        y_train_fold = y.iloc[train_idx]\n",
        "        y_val_fold = y.iloc[val_idx]\n",
        "        \n",
        "        if model_type == 'lgbm':\n",
        "            model = lgb.LGBMClassifier(**params)\n",
        "            model.fit(\n",
        "                X_train_fold, y_train_fold,\n",
        "                eval_set=[(X_val_fold, y_val_fold)],\n",
        "                callbacks=[lgb.early_stopping(150, verbose=False)]\n",
        "            )\n",
        "        elif model_type == 'xgb':\n",
        "            model = xgb.XGBClassifier(**params)\n",
        "            model.fit(\n",
        "                X_train_fold, y_train_fold,\n",
        "                eval_set=[(X_val_fold, y_val_fold)],\n",
        "                verbose=False\n",
        "            )\n",
        "        elif model_type == 'cat':\n",
        "            model = CatBoostClassifier(**params)\n",
        "            model.fit(\n",
        "                X_train_fold, y_train_fold,\n",
        "                eval_set=[(X_val_fold, y_val_fold)],\n",
        "                use_best_model=True,\n",
        "                verbose=False\n",
        "            )\n",
        "        \n",
        "        # Store predictions\n",
        "        oof_preds[val_idx] = model.predict_proba(X_val_fold)\n",
        "        test_preds += model.predict_proba(X_test) / N_FOLDS\n",
        "        \n",
        "        # Clean up memory\n",
        "        del model, X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
        "        gc.collect()\n",
        "    \n",
        "    # Calculate CV score\n",
        "    cv_score = accuracy_score(y, np.argmax(oof_preds, axis=1))\n",
        "    print(f\"  {model_type.upper()} CV Score: {cv_score:.5f}\")\n",
        "    \n",
        "    return oof_preds, test_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters (tuned for high performance)\n",
        "\n",
        "params_lgbm = {\n",
        "    'objective': 'multiclass',\n",
        "    'metric': 'multi_logloss',\n",
        "    'random_state': SEED,\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.02,\n",
        "    'num_leaves': 100,\n",
        "    'max_depth': 12,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 0.1,\n",
        "    'min_child_samples': 20,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "params_xgb = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'random_state': SEED,\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.02,\n",
        "    'max_depth': 10,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'tree_method': 'hist',\n",
        "    'early_stopping_rounds': 150,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "params_cat = {\n",
        "    'loss_function': 'MultiClass',\n",
        "    'random_seed': SEED,\n",
        "    'iterations': 3000,\n",
        "    'learning_rate': 0.03,\n",
        "    'depth': 10,\n",
        "    'l2_leaf_reg': 5,\n",
        "    'bagging_temperature': 0.8,\n",
        "    'od_type': 'Iter',\n",
        "    'od_wait': 150,\n",
        "    'thread_count': -1\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models\n",
        "print(\"Training models...\\n\")\n",
        "\n",
        "all_oof = {}\n",
        "all_preds = {}\n",
        "\n",
        "# LightGBM\n",
        "oof_lgbm, preds_lgbm = train_model(X, y, X_test, params_lgbm, 'lgbm')\n",
        "all_oof['lgbm'] = oof_lgbm\n",
        "all_preds['lgbm'] = preds_lgbm\n",
        "\n",
        "# XGBoost\n",
        "oof_xgb, preds_xgb = train_model(X, y, X_test, params_xgb, 'xgb')\n",
        "all_oof['xgb'] = oof_xgb\n",
        "all_preds['xgb'] = preds_xgb\n",
        "\n",
        "# CatBoost\n",
        "oof_cat, preds_cat = train_model(X, y, X_test, params_cat, 'cat')\n",
        "all_oof['cat'] = oof_cat\n",
        "all_preds['cat'] = preds_cat\n",
        "\n",
        "print(\"\\nâœ“ All models trained successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ensemble Strategy 1: Stacking\n",
        "\n",
        "Stacking uses a meta-learner (Logistic Regression) trained on the out-of-fold predictions from base models. This learns the optimal way to combine model outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate OOF predictions from all models\n",
        "oof_stacked = np.hstack(list(all_oof.values()))\n",
        "preds_stacked = np.hstack(list(all_preds.values()))\n",
        "\n",
        "print(f\"Stacked features shape: {oof_stacked.shape}\")\n",
        "\n",
        "# Train meta-learner\n",
        "meta_model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
        "meta_model.fit(oof_stacked, y)\n",
        "\n",
        "# Generate predictions\n",
        "oof_final_stacking = meta_model.predict_proba(oof_stacked)\n",
        "preds_final_stacking = meta_model.predict_proba(preds_stacked)\n",
        "\n",
        "# Evaluate\n",
        "stacking_score = accuracy_score(y, np.argmax(oof_final_stacking, axis=1))\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"STACKING CV SCORE: {stacking_score:.6f}\")\n",
        "print(f\"{'='*50}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Ensemble Strategy 2: Blending (Weighted Average)\n",
        "\n",
        "Blending combines model predictions using fixed weights. Weights are assigned based on individual model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Blending weights (based on typical model performance)\n",
        "WEIGHTS = {\n",
        "    'lgbm': 0.45,\n",
        "    'cat': 0.35,\n",
        "    'xgb': 0.20\n",
        "}\n",
        "\n",
        "print(f\"Blending weights: {WEIGHTS}\")\n",
        "\n",
        "# Weighted average of predictions\n",
        "oof_final_blending = (\n",
        "    all_oof['lgbm'] * WEIGHTS['lgbm'] +\n",
        "    all_oof['cat'] * WEIGHTS['cat'] +\n",
        "    all_oof['xgb'] * WEIGHTS['xgb']\n",
        ")\n",
        "\n",
        "preds_final_blending = (\n",
        "    all_preds['lgbm'] * WEIGHTS['lgbm'] +\n",
        "    all_preds['cat'] * WEIGHTS['cat'] +\n",
        "    all_preds['xgb'] * WEIGHTS['xgb']\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "blending_score = accuracy_score(y, np.argmax(oof_final_blending, axis=1))\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"BLENDING CV SCORE: {blending_score:.6f}\")\n",
        "print(f\"{'='*50}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Submissions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission files\n",
        "\n",
        "# Stacking submission\n",
        "submission_stacking = sample_submission.copy()\n",
        "submission_stacking[\"Cover_Type\"] = np.argmax(preds_final_stacking, axis=1) + 1\n",
        "stacking_filename = f\"submission_stacking_{stacking_score:.5f}.csv\"\n",
        "submission_stacking.to_csv(stacking_filename, index=False)\n",
        "print(f\"âœ“ Stacking submission saved: {stacking_filename}\")\n",
        "\n",
        "# Blending submission\n",
        "submission_blending = sample_submission.copy()\n",
        "submission_blending[\"Cover_Type\"] = np.argmax(preds_final_blending, axis=1) + 1\n",
        "blending_filename = f\"submission_blending_{blending_score:.5f}.csv\"\n",
        "submission_blending.to_csv(blending_filename, index=False)\n",
        "print(f\"âœ“ Blending submission saved: {blending_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Ensemble Method | CV Score |\n",
        "|-----------------|----------|\n",
        "| Stacking (Logistic Regression Meta-Learner) | See output above |\n",
        "| Blending (Weighted Average) | See output above |\n",
        "\n",
        "**Tips for improving score:**\n",
        "- Tune blending weights using validation data\n",
        "- Add more diverse base models (Neural Networks, ExtraTrees)\n",
        "- Feature selection using permutation importance\n",
        "- Pseudo-labeling with high-confidence test predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
